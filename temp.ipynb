{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from whisperx import load_audio\n",
    "from whisperx.vads import Vad, Silero, Pyannote, SileroCustom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/whisper-large-v3\"\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
    "processor = WhisperProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Performing voice activity detection using Silero...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "vad = SileroCustom(\n",
    "    device=model.device,\n",
    "    chunk_size=1000,\n",
    "    vad_onset=0.3,\n",
    "    vad_offset=0.2,\n",
    "    vad_onnx=True,\n",
    "    silero_merge_cutoff=0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamil.mp3\n",
      "1\n",
      "1\n",
      "kannada.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "hindi2.wav\n",
      "1\n",
      "1\n",
      "hindi.wav\n",
      "1\n",
      "1\n",
      "example.wav\n",
      "3\n",
      "3\n",
      "/home/ubuntu/v2v-voice-library/data/fisher/audios/000/fe_03_00001.wav\n",
      "146\n",
      "146\n",
      "125\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "audios = []\n",
    "\n",
    "def get_vad_segments(waveform, sr):\n",
    "    vad_input = {'waveform': waveform, 'sample_rate': sr}\n",
    "    vad_segments = vad(vad_input)\n",
    "    temp = [segment for i, segment in enumerate(vad_segments)]\n",
    "    print(len(temp))\n",
    "    vad_segments = vad.merge_chunks(\n",
    "        vad_segments,\n",
    "        chunk_size=10,\n",
    "        onset=0.5,\n",
    "        offset=0.383,\n",
    "    )\n",
    "    audios = []\n",
    "    for segment in vad_segments:\n",
    "        start = int(segment['start'] * sr)\n",
    "        end = int(segment['end'] * sr)\n",
    "        audios.append(\n",
    "            waveform[start:end]\n",
    "        )\n",
    "    return audios\n",
    "\n",
    "audios = []\n",
    "for file in os.listdir(\"examples\") + [\n",
    "    '/home/ubuntu/v2v-voice-library/data/fisher/audios/000/fe_03_00001.wav',\n",
    "]:\n",
    "    if file.endswith(\".wav\") or file.endswith(\".mp3\"):\n",
    "        if file == '/home/ubuntu/v2v-voice-library/data/fisher/audios/000/fe_03_00001.wav':\n",
    "            waveform, sr = librosa.load(file, sr=None, mono=False)\n",
    "            if sr != 16000:\n",
    "                waveform = librosa.resample(waveform, orig_sr=sr, target_sr=16000)\n",
    "                sr = 16000\n",
    "            waveform1 = waveform[0]\n",
    "            waveform2 = waveform[1]\n",
    "            print(file)\n",
    "            vad_segments = get_vad_segments(waveform1, sr)\n",
    "            print(len(vad_segments))\n",
    "            audios.extend(vad_segments)\n",
    "            vad_segments = get_vad_segments(waveform2, sr)\n",
    "            print(len(vad_segments))\n",
    "            audios.extend(vad_segments)\n",
    "        else:\n",
    "            print(file)\n",
    "            vad_segments = get_vad_segments(load_audio(f\"examples/{file}\"), 16000)\n",
    "            print(len(vad_segments))\n",
    "            audios.extend(vad_segments)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(audios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperx.audio import N_SAMPLES, SAMPLE_RATE, load_audio, log_mel_spectrogram\n",
    "\n",
    "def preprocess_audio(audio_data):\n",
    "    \"\"\"Preprocess audio data for the model.\"\"\"\n",
    "    # The HF WhisperFeatureExtractor uses 80 mel bins by default\n",
    "    # Access it from the feature_extractor's config\n",
    "    \n",
    "    n_mels = 128  if 'v3' in model_name else 80 # Default value for Whisper models\n",
    "    if hasattr(processor, \"feature_extractor\") and hasattr(processor.feature_extractor, \"config\"):\n",
    "        n_mels = getattr(processor.feature_extractor.config, \"num_mel_bins\", 80)\n",
    "    \n",
    "    features = log_mel_spectrogram(\n",
    "        audio_data,\n",
    "        n_mels=n_mels,\n",
    "        padding=N_SAMPLES - audio_data.shape[0] if audio_data.shape[0] < N_SAMPLES else 0,\n",
    "    )\n",
    "    # Convert features to match model's dtype\n",
    "    return features.to(device=model.device, dtype=model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-3.3992382e-03, -2.3423466e-03, -5.0927320e-04, ...,\n",
      "       -3.0292205e-05,  2.3657558e-06,  4.1739375e-05],\n",
      "      shape=(9668032,), dtype=float32), array([-0.06818755, -0.05907486, -0.03997428, ...,  0.00322126,\n",
      "        0.00317312,  0.00298542], shape=(9534400,), dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input_features = []\n",
    "for audio in audios:\n",
    "    input_features.append(preprocess_audio(audio))\n",
    "\n",
    "# input_features = torch.stack(input_features).to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(input_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12008,  4516,  8094,  ...,    11, 12008,  4516],\n",
      "        [23656, 17167,  2133,  ..., 50257, 50257, 50257],\n",
      "        [34725,   229, 11891,  ..., 50257, 50257, 50257],\n",
      "        ...,\n",
      "        [ 1222,  1315,   311,  ...,  1044,   291,    13],\n",
      "        [ 2425,    11, 13285,  ..., 50257, 50257, 50257],\n",
      "        [ 1012,   366,   291,  ..., 50257, 50257, 50257]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": None,\n",
    "    \"num_beams\": 5,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"temperature\": 0.0,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"length_penalty\": 1.0,\n",
    "}\n",
    "\n",
    "outputs = []\n",
    "for batch in torch.split(input_features, 16):\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            batch,\n",
    "            **gen_kwargs\n",
    "        )\n",
    "    print(output)\n",
    "    torch.cuda.empty_cache()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processor.decode(output[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eating at home. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you\n"
     ]
    }
   ],
   "source": [
    "text = processor.decode(output[-4], skip_special_tokens=False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
