{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Performing voice activity detection using Silero...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/aayush/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "from whisperx.vads import SileroCustom\n",
    "\n",
    "default_vad_options = {\n",
    "        \"chunk_size\": 30, # needed by silero since binarization happens before merge_chunks\n",
    "        \"vad_onset\": 0.500,\n",
    "        \"vad_offset\": 0.363,\n",
    "        \"vad_onnx\": True,\n",
    "        \"silero_merge_cutoff\": 0.1\n",
    "    }\n",
    "\n",
    "vad_model = SileroCustom(**default_vad_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved channel 1 to temp.wav\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "audio_path = '/Users/aayush/nurix/DialogueManager/random/no_turn_ends/dataset_exp/no_turn_ends_audio/fe_03_03242_34.355_42.753.wav'\n",
    "waveform, sample_rate = torchaudio.load(audio_path)\n",
    "channel1 = waveform[0]  # Get the first channel\n",
    "temp_path = 'temp.wav'\n",
    "torchaudio.save(temp_path, channel1.unsqueeze(0), sample_rate)\n",
    "print(f\"Saved channel 1 to {temp_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperx.audio import N_SAMPLES, SAMPLE_RATE, load_audio, log_mel_spectrogram\n",
    "audio_path = 'temp.wav'\n",
    "audio = load_audio(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = vad_model.preprocess_audio(audio)\n",
    "z1 = {\"waveform\": waveform, \"sample_rate\": SAMPLE_RATE}\n",
    "\n",
    "out1 = vad_model(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0.032, 'end': 2.176, 'segments': [(0.032, 2.176)]},\n",
       " {'start': 3.424, 'end': 4.0, 'segments': [(3.424, 4.0)]},\n",
       " {'start': 7.296, 'end': 8.416, 'segments': [(7.296, 8.416)]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vad_model.merge_chunks(out1, 30, 0.5, 0.363)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8828125e-04, -3.0517578e-05,  4.8828125e-04, ...,\n",
       "        1.4038086e-03, -2.4414062e-04, -1.1291504e-03], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': array([ 0.00024414,  0.00027466,  0.00024414, ...,  0.00140381,\n",
      "       -0.00024414, -0.00112915], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def data(audio, segments):\n",
    "    for seg in segments:\n",
    "        f1 = int(seg['start'] * SAMPLE_RATE)\n",
    "        f2 = int(seg['end'] * SAMPLE_RATE)\n",
    "        # print(f2-f1)\n",
    "        yield {'inputs': audio[f1:f2]}\n",
    "        \n",
    "for i in data(audio, merged_segments):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_activity = predicts > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Convert voice activity tensor to time segments\"\"\"\n",
    "segments = []\n",
    "start_idx = None\n",
    "\n",
    "# Find continuous voice segments\n",
    "for i, is_voice in enumerate(voice_activity):\n",
    "    if is_voice and start_idx is None:\n",
    "        start_idx = i\n",
    "    elif not is_voice and start_idx is not None:\n",
    "        # Convert chunk indices to seconds (each chunk is 32ms)\n",
    "        start_time = round(start_idx * 32 / 1000, 3)\n",
    "        end_time = round(i * 32 / 1000, 3)\n",
    "        \n",
    "        # Store segment without applying max duration constraint\n",
    "        segments.append((start_time, end_time))\n",
    "        start_idx = None\n",
    "\n",
    "# Handle case where voice activity continues until end\n",
    "if start_idx is not None:\n",
    "    start_time = round(start_idx * 32 / 1000, 3)\n",
    "    end_time = round(len(voice_activity) * 32 / 1000, 3)\n",
    "    segments.append((start_time, end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.032, 1.504), (1.6, 2.176), (3.424, 4.0), (7.296, 7.712), (7.744, 8.416)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vad_segments \u001b[38;5;241m=\u001b[39m \u001b[43mvad_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwaveform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAMPLE_RATE\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nurix/DialogueManager/whisperX/whisperx/vads/silero_custom.py:51\u001b[0m, in \u001b[0;36mSileroCustom.__call__\u001b[0;34m(self, audio, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Handle mono audio (single channel)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m voice_activity \u001b[38;5;241m=\u001b[39m predicts \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvad_onset\n\u001b[0;32m---> 51\u001b[0m segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_segments_from_activity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoice_activity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [SegmentX(start, end, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m segments]\n",
      "File \u001b[0;32m~/nurix/DialogueManager/whisperX/whisperx/vads/silero_custom.py:62\u001b[0m, in \u001b[0;36mSileroCustom._get_segments_from_activity\u001b[0;34m(self, voice_activity)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Find continuous voice segments\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, is_voice \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(voice_activity):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_voice \u001b[38;5;129;01mand\u001b[39;00m start_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m         start_idx \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_voice \u001b[38;5;129;01mand\u001b[39;00m start_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;66;03m# Convert chunk indices to seconds (each chunk is 32ms)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "vad_segments = vad_model({\"waveform\": waveform, \"sample_rate\": SAMPLE_RATE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aayush",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
